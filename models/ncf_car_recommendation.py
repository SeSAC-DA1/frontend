"""
NCF (Neural Collaborative Filtering) for CarFinanceAI
Based on "Neural Collaborative Filtering" (He et al. 2017)
Specialized for vehicle recommendation with contextual features
"""

import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
import json

class CarRecommendationNCF:
    """
    NCF ëª¨ë¸ì„ ì°¨ëŸ‰ ì¶”ì²œì— íŠ¹í™”ì‹œí‚¨ êµ¬í˜„

    Features:
    - GMF + MLP í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì¡°
    - ì°¨ëŸ‰ íŠ¹ì„± í”¼ì²˜ í†µí•©
    - ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ë°˜ì˜
    - ì‹¤ì‹œê°„ ì¶”ë¡  ìµœì í™”
    """

    def __init__(self,
                 num_users: int = 1000,
                 num_vehicles: int = 500,
                 embedding_dim: int = 64,
                 mlp_layers: List[int] = [128, 64, 32],
                 dropout_rate: float = 0.2,
                 l2_reg: float = 1e-6):

        self.num_users = num_users
        self.num_vehicles = num_vehicles
        self.embedding_dim = embedding_dim
        self.mlp_layers = mlp_layers
        self.dropout_rate = dropout_rate
        self.l2_reg = l2_reg

        # ì°¨ëŸ‰ íŠ¹ì„± ì°¨ì› ì •ì˜
        self.user_feature_dim = 10  # ë‚˜ì´, ì†Œë“, ì§€ì—­ ë“±
        self.vehicle_feature_dim = 15  # ê°€ê²©, ì—°ì‹, ë¸Œëœë“œ ë“±
        self.context_feature_dim = 5   # ê³„ì ˆ, ì‹œê°„, ì‹œì¥ìƒí™© ë“±

        self.model = None
        self.training_history = None

    def build_model(self):
        """NCF ëª¨ë¸ êµ¬ì¶• - CarFinanceAI íŠ¹í™”"""

        # ===== ì…ë ¥ ë ˆì´ì–´ =====
        # ê¸°ë³¸ user-item ID
        user_input = Input(shape=(), name='user_id', dtype='int32')
        vehicle_input = Input(shape=(), name='vehicle_id', dtype='int32')

        # ì¶”ê°€ íŠ¹ì„± í”¼ì²˜ë“¤
        user_features = Input(shape=(self.user_feature_dim,), name='user_features')
        vehicle_features = Input(shape=(self.vehicle_feature_dim,), name='vehicle_features')
        context_features = Input(shape=(self.context_feature_dim,), name='context_features')

        # ===== GMF ë¸Œëœì¹˜ (Generalized Matrix Factorization) =====
        # ì‚¬ìš©ì/ì°¨ëŸ‰ ì„ë² ë”©
        user_embedding_gmf = Embedding(
            self.num_users, self.embedding_dim,
            embeddings_regularizer=l2(self.l2_reg),
            name='user_embedding_gmf'
        )(user_input)

        vehicle_embedding_gmf = Embedding(
            self.num_vehicles, self.embedding_dim,
            embeddings_regularizer=l2(self.l2_reg),
            name='vehicle_embedding_gmf'
        )(vehicle_input)

        # GMF: element-wise product
        gmf_user_vec = Flatten()(user_embedding_gmf)
        gmf_vehicle_vec = Flatten()(vehicle_embedding_gmf)
        gmf_output = Multiply(name='gmf_multiply')([gmf_user_vec, gmf_vehicle_vec])

        # ===== MLP ë¸Œëœì¹˜ (Multi-Layer Perceptron) =====
        # ì‚¬ìš©ì/ì°¨ëŸ‰ ì„ë² ë”© (MLPìš© - ë‹¤ë¥¸ ê°€ì¤‘ì¹˜)
        user_embedding_mlp = Embedding(
            self.num_users, self.embedding_dim,
            embeddings_regularizer=l2(self.l2_reg),
            name='user_embedding_mlp'
        )(user_input)

        vehicle_embedding_mlp = Embedding(
            self.num_vehicles, self.embedding_dim,
            embeddings_regularizer=l2(self.l2_reg),
            name='vehicle_embedding_mlp'
        )(vehicle_input)

        # MLP: concatenation + dense layers
        mlp_user_vec = Flatten()(user_embedding_mlp)
        mlp_vehicle_vec = Flatten()(vehicle_embedding_mlp)

        # ëª¨ë“  í”¼ì²˜ ê²°í•©
        mlp_input = Concatenate(name='mlp_concat')([
            mlp_user_vec,
            mlp_vehicle_vec,
            user_features,      # ì‚¬ìš©ì ì¸êµ¬í†µê³„í•™ì  ì •ë³´
            vehicle_features,   # ì°¨ëŸ‰ ì„¸ë¶€ íŠ¹ì„±
            context_features    # ìƒí™© ì •ë³´
        ])

        # MLP ë ˆì´ì–´ë“¤
        mlp_output = mlp_input
        for i, layer_size in enumerate(self.mlp_layers):
            mlp_output = Dense(
                layer_size,
                activation='relu',
                kernel_regularizer=l2(self.l2_reg),
                name=f'mlp_layer_{i+1}'
            )(mlp_output)
            mlp_output = Dropout(self.dropout_rate)(mlp_output)

        # ===== ìµœì¢… ê²°í•© ë ˆì´ì–´ =====
        # GMF + MLP ê²°í•©
        final_input = Concatenate(name='final_concat')([gmf_output, mlp_output])

        # ìµœì¢… ì˜ˆì¸¡ ë ˆì´ì–´
        prediction = Dense(
            1,
            activation='sigmoid',  # 0-1 ì„ í˜¸ë„ ì ìˆ˜
            kernel_regularizer=l2(self.l2_reg),
            name='prediction'
        )(final_input)

        # ëª¨ë¸ ì •ì˜
        self.model = Model(
            inputs=[user_input, vehicle_input, user_features, vehicle_features, context_features],
            outputs=prediction,
            name='CarFinanceAI_NCF'
        )

        return self.model

    def compile_model(self, learning_rate: float = 0.001):
        """ëª¨ë¸ ì»´íŒŒì¼"""
        if self.model is None:
            self.build_model()

        self.model.compile(
            optimizer=Adam(learning_rate=learning_rate),
            loss='binary_crossentropy',  # ì„ í˜¸/ë¹„ì„ í˜¸ ì´ì§„ ë¶„ë¥˜
            metrics=['accuracy', 'precision', 'recall']
        )

        return self.model

    def prepare_training_data(self, interaction_df: pd.DataFrame) -> Dict:
        """í›ˆë ¨ ë°ì´í„° ì¤€ë¹„"""

        # ê¸ì •ì  ìƒí˜¸ì‘ìš© (í‰ì  4+ ë˜ëŠ” êµ¬ë§¤/ë¬¸ì˜)
        positive_interactions = interaction_df[
            (interaction_df['rating'] >= 4.0) |
            (interaction_df['interaction_type'].isin(['purchase', 'inquiry', 'favorite']))
        ]

        # ë¶€ì •ì  ìƒ˜í”Œë§ (ëœë¤ ìƒ˜í”Œë§)
        negative_samples = self._negative_sampling(
            positive_interactions,
            ratio=4  # ê¸ì •:ë¶€ì • = 1:4
        )

        # í›ˆë ¨ ë°ì´í„° ê²°í•©
        train_data = pd.concat([positive_interactions, negative_samples])
        train_data = train_data.sample(frac=1).reset_index(drop=True)  # ì…í”Œ

        # í”¼ì²˜ ì¶”ì¶œ
        training_data = {
            'user_ids': train_data['user_id'].values,
            'vehicle_ids': train_data['vehicle_id'].values,
            'user_features': self._extract_user_features(train_data),
            'vehicle_features': self._extract_vehicle_features(train_data),
            'context_features': self._extract_context_features(train_data),
            'labels': train_data['preference_score'].values  # 0 or 1
        }

        return training_data

    def _negative_sampling(self, positive_df: pd.DataFrame, ratio: int = 4) -> pd.DataFrame:
        """ë¶€ì •ì  ìƒ˜í”Œ ìƒì„±"""
        negative_samples = []

        for _, pos_sample in positive_df.iterrows():
            user_id = pos_sample['user_id']

            # í•´ë‹¹ ì‚¬ìš©ìê°€ ìƒí˜¸ì‘ìš©í•˜ì§€ ì•Šì€ ì°¨ëŸ‰ë“¤ ì¤‘ ëœë¤ ì„ íƒ
            user_vehicles = set(positive_df[positive_df['user_id'] == user_id]['vehicle_id'])
            all_vehicles = set(range(self.num_vehicles))
            uninteracted_vehicles = list(all_vehicles - user_vehicles)

            # ëœë¤ ìƒ˜í”Œë§
            if len(uninteracted_vehicles) >= ratio:
                sampled_vehicles = np.random.choice(uninteracted_vehicles, ratio, replace=False)

                for vehicle_id in sampled_vehicles:
                    negative_sample = pos_sample.copy()
                    negative_sample['vehicle_id'] = vehicle_id
                    negative_sample['preference_score'] = 0  # ë¶€ì •ì  ë ˆì´ë¸”
                    negative_samples.append(negative_sample)

        return pd.DataFrame(negative_samples)

    def _extract_user_features(self, df: pd.DataFrame) -> np.ndarray:
        """ì‚¬ìš©ì íŠ¹ì„± í”¼ì²˜ ì¶”ì¶œ"""
        # ì‚¬ìš©ì ì¸êµ¬í†µê³„í•™ì  ì •ë³´
        user_features = []

        for _, row in df.iterrows():
            features = [
                row.get('user_age', 30) / 100.0,           # ë‚˜ì´ ì •ê·œí™”
                row.get('user_income', 5000) / 10000.0,    # ì†Œë“ ì •ê·œí™”
                row.get('user_family_size', 2) / 10.0,     # ê°€ì¡± ìˆ˜
                row.get('user_driving_exp', 5) / 50.0,     # ìš´ì „ ê²½ë ¥
                row.get('user_location_code', 0) / 100.0,  # ì§€ì—­ ì½”ë“œ
                row.get('user_education', 3) / 5.0,        # í•™ë ¥ ìˆ˜ì¤€
                row.get('user_occupation_code', 0) / 20.0, # ì§ì—… ì½”ë“œ
                row.get('user_gender', 0),                 # ì„±ë³„ (0/1)
                row.get('user_married', 0),                # ê²°í˜¼ ì—¬ë¶€
                row.get('user_car_ownership', 0)           # ì°¨ëŸ‰ ë³´ìœ  ì—¬ë¶€
            ]
            user_features.append(features)

        return np.array(user_features, dtype=np.float32)

    def _extract_vehicle_features(self, df: pd.DataFrame) -> np.ndarray:
        """ì°¨ëŸ‰ íŠ¹ì„± í”¼ì²˜ ì¶”ì¶œ"""
        vehicle_features = []

        for _, row in df.iterrows():
            features = [
                row.get('vehicle_price', 3000) / 10000.0,     # ê°€ê²© ì •ê·œí™”
                row.get('vehicle_year', 2020) / 2025.0,       # ì—°ì‹ ì •ê·œí™”
                row.get('vehicle_mileage', 50000) / 200000.0, # ì£¼í–‰ê±°ë¦¬
                row.get('vehicle_engine_size', 2.0) / 5.0,    # ë°°ê¸°ëŸ‰
                row.get('vehicle_fuel_efficiency', 12) / 30.0, # ì—°ë¹„
                row.get('vehicle_safety_rating', 4) / 5.0,    # ì•ˆì „ë“±ê¸‰
                row.get('vehicle_brand_rank', 5) / 20.0,      # ë¸Œëœë“œ ìˆœìœ„
                row.get('vehicle_body_type_code', 0) / 10.0,  # ì°¨ì¢… ì½”ë“œ
                row.get('vehicle_fuel_type_code', 0) / 5.0,   # ì—°ë£Œ íƒ€ì…
                row.get('vehicle_transmission_auto', 1),      # ìë™ë³€ì†ê¸° ì—¬ë¶€
                row.get('vehicle_accident_history', 0),      # ì‚¬ê³  ì´ë ¥
                row.get('vehicle_owner_count', 1) / 5.0,     # ì´ì „ ì†Œìœ ì ìˆ˜
                row.get('vehicle_maintenance_score', 3) / 5.0, # ì •ë¹„ ìƒíƒœ
                row.get('vehicle_popularity_score', 0.5),     # ì¸ê¸°ë„
                row.get('vehicle_resale_value', 0.7)          # ë¦¬ì„¸ì¼ ê°€ì¹˜
            ]
            vehicle_features.append(features)

        return np.array(vehicle_features, dtype=np.float32)

    def _extract_context_features(self, df: pd.DataFrame) -> np.ndarray:
        """ìƒí™© ì •ë³´ í”¼ì²˜ ì¶”ì¶œ"""
        context_features = []

        for _, row in df.iterrows():
            features = [
                row.get('season_code', 0) / 4.0,        # ê³„ì ˆ (0-3)
                row.get('hour_of_day', 12) / 24.0,      # ì‹œê°„ëŒ€
                row.get('day_of_week', 3) / 7.0,        # ìš”ì¼
                row.get('market_condition', 0.5),       # ì‹œì¥ ìƒí™© ì§€ìˆ˜
                row.get('oil_price_index', 0.5)         # ìœ ê°€ ì§€ìˆ˜
            ]
            context_features.append(features)

        return np.array(context_features, dtype=np.float32)

    def train(self,
              interaction_df: pd.DataFrame,
              validation_split: float = 0.2,
              epochs: int = 50,
              batch_size: int = 256,
              early_stopping_patience: int = 10):
        """ëª¨ë¸ í›ˆë ¨"""

        # ë°ì´í„° ì¤€ë¹„
        print("ğŸ“Š í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        training_data = self.prepare_training_data(interaction_df)

        # ëª¨ë¸ ì»´íŒŒì¼
        if self.model is None:
            self.compile_model()

        # ì½œë°± ì„¤ì •
        callbacks = [
            tf.keras.callbacks.EarlyStopping(
                monitor='val_loss',
                patience=early_stopping_patience,
                restore_best_weights=True
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=1e-7
            )
        ]

        # í›ˆë ¨ ì‹¤í–‰
        print("ğŸš€ NCF ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        self.training_history = self.model.fit(
            x={
                'user_id': training_data['user_ids'],
                'vehicle_id': training_data['vehicle_ids'],
                'user_features': training_data['user_features'],
                'vehicle_features': training_data['vehicle_features'],
                'context_features': training_data['context_features']
            },
            y=training_data['labels'],
            validation_split=validation_split,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )

        print("âœ… í›ˆë ¨ ì™„ë£Œ!")
        return self.training_history

    def predict_user_preferences(self,
                                user_id: int,
                                vehicle_ids: List[int],
                                user_features: np.ndarray,
                                context_features: np.ndarray) -> np.ndarray:
        """ì‚¬ìš©ì-ì°¨ëŸ‰ ì„ í˜¸ë„ ì˜ˆì¸¡"""

        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # ì°¨ëŸ‰ íŠ¹ì„± ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” DBì—ì„œ ê°€ì ¸ì˜´)
        vehicle_features = self._get_vehicle_features_batch(vehicle_ids)

        # ë°°ì¹˜ ì˜ˆì¸¡
        batch_size = len(vehicle_ids)
        predictions = self.model.predict({
            'user_id': np.array([user_id] * batch_size),
            'vehicle_id': np.array(vehicle_ids),
            'user_features': np.tile(user_features, (batch_size, 1)),
            'vehicle_features': vehicle_features,
            'context_features': np.tile(context_features, (batch_size, 1))
        })

        return predictions.flatten()

    def get_top_recommendations(self,
                              user_id: int,
                              user_features: np.ndarray,
                              context_features: np.ndarray,
                              candidate_vehicles: List[int],
                              top_k: int = 10) -> List[Dict]:
        """Top-K ì¶”ì²œ ìƒì„±"""

        # ì„ í˜¸ë„ ì˜ˆì¸¡
        preferences = self.predict_user_preferences(
            user_id, candidate_vehicles, user_features, context_features
        )

        # ìƒìœ„ Kê°œ ì„ íƒ
        top_indices = np.argsort(preferences)[::-1][:top_k]
        top_vehicles = [candidate_vehicles[i] for i in top_indices]
        top_scores = [preferences[i] for i in top_indices]

        # ì¶”ì²œ ê²°ê³¼ í¬ë§·
        recommendations = []
        for vehicle_id, score in zip(top_vehicles, top_scores):
            recommendations.append({
                'vehicle_id': vehicle_id,
                'preference_score': float(score),
                'reasoning': self._explain_recommendation(user_id, vehicle_id)
            })

        return recommendations

    def get_recommendations(self, user_dict: Dict, n_recommendations: int = 10) -> List[Dict]:
        """
        FastAPI í†µí•©ì„ ìœ„í•œ ê°„ì†Œí™”ëœ ì¶”ì²œ ì¸í„°í˜ì´ìŠ¤
        """
        try:
            # Mock ì‚¬ìš©ì íŠ¹ì„± ìƒì„± (ì‹¤ì œë¡œëŠ” user_dictì—ì„œ ì¶”ì¶œ)
            user_features = np.array([[
                user_dict.get('age', 30) / 100.0,
                user_dict.get('income', 5000) / 10000.0,
                2.0 / 10.0,  # family_size
                5.0 / 50.0,  # driving_exp
                0.0 / 100.0, # location_code
                3.0 / 5.0,   # education
                0.0 / 20.0,  # occupation_code
                0.0,         # gender
                0.0,         # married
                0.0          # car_ownership
            ]], dtype=np.float32)

            # Mock ì»¨í…ìŠ¤íŠ¸ íŠ¹ì„±
            context_features = np.array([[
                1.0 / 4.0,   # season
                12.0 / 24.0, # hour
                3.0 / 7.0,   # day_of_week
                0.5,         # market_condition
                0.5          # oil_price_index
            ]], dtype=np.float32)

            # Mock í›„ë³´ ì°¨ëŸ‰ ëª©ë¡ (ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê°€ì ¸ì˜´)
            budget_max = user_dict.get('budget_max', 5000)
            candidate_vehicles = list(range(min(50, self.num_vehicles)))

            # Mock ì¶”ì²œ ê²°ê³¼ ìƒì„± (ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì€ ê²½ìš°)
            if self.model is None:
                recommendations = []
                for i, vehicle_id in enumerate(candidate_vehicles[:n_recommendations]):
                    score = 0.9 - (i * 0.05)  # ì ì§„ì  ê°ì†Œ
                    recommendations.append({
                        'vehicle_id': f"ncf_{vehicle_id}",
                        'score': max(0.3, score),
                        'confidence': 0.85,
                        'reasons': [f'NCF Algorithm - ì°¨ëŸ‰ {vehicle_id}', 'ë”¥ëŸ¬ë‹ ê¸°ë°˜ í˜‘ì—… í•„í„°ë§'],
                        'algorithm': 'Neural Collaborative Filtering'
                    })
                return recommendations

            # ì‹¤ì œ NCF ëª¨ë¸ì„ ì‚¬ìš©í•œ ì¶”ì²œ
            user_id_hash = hash(user_dict.get('user_id', 'default')) % self.num_users

            detailed_recs = self.get_top_recommendations(
                user_id=user_id_hash,
                user_features=user_features[0],
                context_features=context_features[0],
                candidate_vehicles=candidate_vehicles,
                top_k=n_recommendations
            )

            # í˜•ì‹ ë³€í™˜
            recommendations = []
            for i, rec in enumerate(detailed_recs):
                recommendations.append({
                    'vehicle_id': str(rec['vehicle_id']),
                    'score': rec['preference_score'],
                    'confidence': 0.88,
                    'reasons': [rec['reasoning'], 'Neural Collaborative Filtering'],
                    'algorithm': 'NCF (He et al. 2017)'
                })

            return recommendations

        except Exception as e:
            # ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ë³¸ Mock ì¶”ì²œ ë°˜í™˜
            print(f"NCF ì¶”ì²œ ì—ëŸ¬: {e}")
            mock_recommendations = []
            for i in range(min(n_recommendations, 10)):
                mock_recommendations.append({
                    'vehicle_id': f"mock_{i}",
                    'score': 0.8 - (i * 0.05),
                    'confidence': 0.75,
                    'reasons': ['NCF Fallback', 'ê¸°ë³¸ ì¶”ì²œ ì‹œìŠ¤í…œ'],
                    'algorithm': 'NCF Mock'
                })
            return mock_recommendations

    def _get_vehicle_features_batch(self, vehicle_ids: List[int]) -> np.ndarray:
        """ì°¨ëŸ‰ íŠ¹ì„± ë°°ì¹˜ ì¡°íšŒ (ì‹¤ì œë¡œëŠ” DB ì¿¼ë¦¬)"""
        # ì„ì‹œ êµ¬í˜„ - ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ
        batch_features = []
        for vehicle_id in vehicle_ids:
            # Mock ì°¨ëŸ‰ íŠ¹ì„±
            features = [
                0.3,   # ê°€ê²©
                0.8,   # ì—°ì‹
                0.4,   # ì£¼í–‰ê±°ë¦¬
                0.6,   # ë°°ê¸°ëŸ‰
                0.7,   # ì—°ë¹„
                0.9,   # ì•ˆì „ë“±ê¸‰
                0.6,   # ë¸Œëœë“œ
                0.5,   # ì°¨ì¢…
                0.3,   # ì—°ë£Œíƒ€ì…
                1.0,   # ìë™ë³€ì†ê¸°
                0.0,   # ì‚¬ê³ ì´ë ¥
                0.2,   # ì†Œìœ ììˆ˜
                0.8,   # ì •ë¹„ìƒíƒœ
                0.6,   # ì¸ê¸°ë„
                0.7    # ë¦¬ì„¸ì¼ê°€ì¹˜
            ]
            batch_features.append(features)

        return np.array(batch_features, dtype=np.float32)

    def _explain_recommendation(self, user_id: int, vehicle_id: int) -> str:
        """ì¶”ì²œ ì´ìœ  ì„¤ëª… ìƒì„±"""
        explanations = [
            "ì„ í˜¸ë„ íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ë†’ì€ ê´€ì‹¬ë„ ì˜ˆìƒ",
            "ìœ ì‚¬í•œ ì‚¬ìš©ìë“¤ì˜ ì„ íƒ íŒ¨í„´",
            "ê°œì¸ ë§ì¶¤ íŠ¹ì„± ë¶„ì„ ê¸°ë°˜",
            "ì‹œì¥ íŠ¸ë Œë“œ ë° ê°€ì„±ë¹„ ì¢…í•© ê³ ë ¤"
        ]
        return explanations[hash(f"{user_id}_{vehicle_id}") % len(explanations)]

    def save_model(self, filepath: str):
        """ëª¨ë¸ ì €ì¥"""
        if self.model is not None:
            self.model.save_weights(filepath)

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                'num_users': self.num_users,
                'num_vehicles': self.num_vehicles,
                'embedding_dim': self.embedding_dim,
                'mlp_layers': self.mlp_layers
            }

            with open(f"{filepath}_metadata.json", 'w') as f:
                json.dump(metadata, f)

        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")

    def load_model(self, filepath: str):
        """ëª¨ë¸ ë¡œë“œ"""
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(f"{filepath}_metadata.json", 'r') as f:
            metadata = json.load(f)

        # ëª¨ë¸ ì¬êµ¬ì„±
        self.__init__(**metadata)
        self.build_model()
        self.compile_model()

        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        self.model.load_weights(filepath)
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {filepath}")

# NCF í†µí•©ì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def create_mock_interaction_data(num_users: int = 1000, num_vehicles: int = 500) -> pd.DataFrame:
    """Mock ìƒí˜¸ì‘ìš© ë°ì´í„° ìƒì„±"""
    np.random.seed(42)

    interactions = []

    for user_id in range(num_users):
        # ì‚¬ìš©ìë³„ 15-30ê°œ ìƒí˜¸ì‘ìš© ìƒì„±
        num_interactions = np.random.randint(15, 31)

        for _ in range(num_interactions):
            vehicle_id = np.random.randint(0, num_vehicles)

            # ìƒí˜¸ì‘ìš© íƒ€ì…ë³„ ì„ í˜¸ë„ ì ìˆ˜
            interaction_type = np.random.choice([
                'view', 'click', 'favorite', 'inquiry', 'purchase'
            ], p=[0.5, 0.3, 0.1, 0.08, 0.02])

            # íƒ€ì…ë³„ ì„ í˜¸ë„ ì ìˆ˜ ë° ëª…ì‹œì  í‰ì 
            if interaction_type == 'purchase':
                preference_score = 1
                rating = np.random.choice([4, 5], p=[0.3, 0.7])
            elif interaction_type == 'inquiry':
                preference_score = 1 if np.random.random() > 0.3 else 0
                rating = np.random.choice([3, 4, 5], p=[0.2, 0.4, 0.4])
            elif interaction_type == 'favorite':
                preference_score = 1
                rating = np.random.choice([4, 5], p=[0.5, 0.5])
            else:
                preference_score = 1 if np.random.random() > 0.7 else 0
                rating = np.random.choice([1, 2, 3, 4, 5], p=[0.1, 0.2, 0.4, 0.2, 0.1])

            interaction = {
                'user_id': user_id,
                'vehicle_id': vehicle_id,
                'interaction_type': interaction_type,
                'preference_score': preference_score,
                'rating': rating,
                'timestamp': pd.Timestamp.now() - pd.Timedelta(days=np.random.randint(0, 365)),

                # ì‚¬ìš©ì íŠ¹ì„± (Mock)
                'user_age': np.random.randint(20, 65),
                'user_income': np.random.randint(3000, 8000),
                'user_family_size': np.random.randint(1, 5),

                # ì°¨ëŸ‰ íŠ¹ì„± (Mock)
                'vehicle_price': np.random.randint(1500, 8000),
                'vehicle_year': np.random.randint(2015, 2024),
                'vehicle_mileage': np.random.randint(10000, 150000),

                # ì»¨í…ìŠ¤íŠ¸ (Mock)
                'season_code': np.random.randint(0, 4),
                'hour_of_day': np.random.randint(0, 24)
            }

            interactions.append(interaction)

    return pd.DataFrame(interactions)

def integrate_ncf_with_fastapi():
    """FastAPIì— NCF ëª¨ë¸ í†µí•©"""
    return """
    # backend/main.pyì— ì¶”ê°€í•  ì½”ë“œ

    from models.ncf_car_recommendation import CarRecommendationNCF
    import numpy as np

    # ì „ì—­ NCF ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
    ncf_model = None

    def load_ncf_model():
        global ncf_model
        try:
            ncf_model = CarRecommendationNCF(
                num_users=10000,
                num_vehicles=5000,
                embedding_dim=64
            )
            ncf_model.load_model('models/trained_ncf_model')
            print("âœ… NCF ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            print(f"âš ï¸ NCF ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            ncf_model = None

    @app.post("/api/recommendations/ncf")
    async def get_ncf_recommendations(request: RecommendationRequest):
        '''NCF ê¸°ë°˜ ì •ë°€ ì¶”ì²œ API'''

        if ncf_model is None:
            load_ncf_model()

        if ncf_model is None:
            raise HTTPException(status_code=503, detail="NCF ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        try:
            # ì‚¬ìš©ì íŠ¹ì„± ì¶”ì¶œ
            user_features = extract_user_features(request.user_profile)
            context_features = extract_context_features()

            # í›„ë³´ ì°¨ëŸ‰ ëª©ë¡ (ì‹¤ì œë¡œëŠ” DB ì¿¼ë¦¬)
            candidate_vehicles = get_candidate_vehicles(request.user_profile)

            # NCF ì¶”ì²œ ì‹¤í–‰
            recommendations = ncf_model.get_top_recommendations(
                user_id=hash(request.user_profile.user_id) % 10000,
                user_features=user_features,
                context_features=context_features,
                candidate_vehicles=candidate_vehicles,
                top_k=request.limit
            )

            return {
                "recommendations": recommendations,
                "algorithm": "Neural Collaborative Filtering (He et al. 2017)",
                "model_type": "Deep Learning Hybrid",
                "confidence": "high"
            }

        except Exception as e:
            raise HTTPException(status_code=500, detail=f"NCF ì¶”ì²œ ì‹¤íŒ¨: {str(e)}")
    """

if __name__ == "__main__":
    # NCF ëª¨ë¸ í…ŒìŠ¤íŠ¸
    print("ğŸ§  CarFinanceAI NCF ëª¨ë¸ í…ŒìŠ¤íŠ¸")

    # Mock ë°ì´í„° ìƒì„±
    interaction_data = create_mock_interaction_data(1000, 500)
    print(f"ğŸ“Š ìƒì„±ëœ ìƒí˜¸ì‘ìš© ë°ì´í„°: {len(interaction_data)} ê±´")

    # NCF ëª¨ë¸ ì´ˆê¸°í™”
    ncf_model = CarRecommendationNCF(
        num_users=1000,
        num_vehicles=500,
        embedding_dim=64,
        mlp_layers=[128, 64, 32]
    )

    # ëª¨ë¸ êµ¬ì¡° í™•ì¸
    model = ncf_model.build_model()
    print(f"ğŸ—ï¸ NCF ëª¨ë¸ êµ¬ì¡°:")
    model.summary()

    print("\nâœ… NCF ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ! ì‹¤ì œ ë°ì´í„°ë¡œ í›ˆë ¨ ê°€ëŠ¥í•©ë‹ˆë‹¤.")