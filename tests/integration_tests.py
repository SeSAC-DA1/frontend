# -*- coding: utf-8 -*-
"""
í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ë° ê²€ì¦
PostgreSQL, ì‹¤ì‹œê°„ í•™ìŠµ, Gemini ë©€í‹°ì—ì´ì „íŠ¸, MCP ì„œë²„ í†µí•© í…ŒìŠ¤íŠ¸
"""

import asyncio
import logging
import json
import time
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
import pytest
import os
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ë‚´ë¶€ ëª¨ë“ˆ
from database.connection import DatabaseManager, UserBehaviorTracker
from database.realtime_learning import get_realtime_engine, get_recommendation_cache
from agents.gemini_recommendation_agent import get_gemini_multi_agent_system
from mcp.carfinance_mcp_server import get_carfinance_mcp_server
from strategies.user_behavior_strategy import get_user_behavior_collection_strategy

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class IntegrationTestSuite:
    """í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸"""

    def __init__(self):
        self.test_results = {}
        self.test_user_id = "test_user_integration_001"
        self.test_vehicle_id = "test_vehicle_001"
        self.db_manager = None
        self.start_time = None

    async def run_all_tests(self) -> Dict[str, Any]:
        """ëª¨ë“  í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.start_time = time.time()
        logger.info("ğŸš€ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")

        try:
            # 1. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸
            await self._test_database_connection()

            # 2. ì‹¤ì‹œê°„ í•™ìŠµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
            await self._test_realtime_learning_system()

            # 3. Gemini ë©€í‹°ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸
            await self._test_gemini_multiagent()

            # 4. MCP ì„œë²„ í…ŒìŠ¤íŠ¸
            await self._test_mcp_server()

            # 5. ì‚¬ìš©ì í–‰ë™ ìˆ˜ì§‘ ì „ëµ í…ŒìŠ¤íŠ¸
            await self._test_behavior_collection_strategy()

            # 6. ì „ì²´ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸
            await self._test_full_system_integration()

            # 7. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            await self._test_system_performance()

            # 8. ì •ë¦¬ ì‘ì—…
            await self._cleanup_test_data()

            # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¢…í•©
            total_time = time.time() - self.start_time
            self.test_results["overall"] = {
                "status": "completed",
                "total_duration_seconds": total_time,
                "timestamp": datetime.now().isoformat()
            }

            logger.info(f"âœ… í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ ({total_time:.2f}ì´ˆ)")
            return self.test_results

        except Exception as e:
            logger.error(f"âŒ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            self.test_results["overall"] = {
                "status": "failed",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
            return self.test_results

    async def _test_database_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        test_name = "database_connection"
        logger.info(f"ğŸ”Œ {test_name} í…ŒìŠ¤íŠ¸ ì‹œì‘")

        try:
            start_time = time.time()

            # DatabaseManager ì´ˆê¸°í™”
            self.db_manager = DatabaseManager()

            # ì—°ê²° í…ŒìŠ¤íŠ¸
            async with self.db_manager.get_connection() as conn:
                result = await conn.fetchval("SELECT 1")
                assert result == 1, "ê¸°ë³¸ ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨"

            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            async with self.db_manager.get_connection() as conn:
                tables = await conn.fetch("""
                    SELECT table_name FROM information_schema.tables
                    WHERE table_schema = 'public'
                    AND table_name IN ('users', 'vehicles', 'user_interactions')
                """)

                table_names = [row['table_name'] for row in tables]
                required_tables = ['users', 'vehicles', 'user_interactions']

                for table in required_tables:
                    if table not in table_names:
                        logger.warning(f"âš ï¸ í•„ìˆ˜ í…Œì´ë¸” ëˆ„ë½: {table}")

            # UserBehaviorTracker í…ŒìŠ¤íŠ¸
            behavior_tracker = UserBehaviorTracker(self.db_manager)

            test_interaction = {
                "user_id": self.test_user_id,
                "vehicle_id": self.test_vehicle_id,
                "interaction_type": "view",
                "context": {"test": True}
            }

            success = await behavior_tracker.track_user_interaction(test_interaction)
            assert success, "ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ì¶”ì  ì‹¤íŒ¨"

            duration = time.time() - start_time
            self.test_results[test_name] = {
                "status": "passed",
                "duration_seconds": duration,
                "details": {
                    "connection_successful": True,
                    "tables_checked": len(required_tables),
                    "behavior_tracking": success
                }
            }

            logger.info(f"âœ… {test_name} í…ŒìŠ¤íŠ¸ í†µê³¼ ({duration:.2f}ì´ˆ)")

        except Exception as e:
            self.test_results[test_name] = {
                "status": "failed",
                "error": str(e),
                "duration_seconds": time.time() - start_time
            }
            logger.error(f"âŒ {test_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            raise

    async def _test_realtime_learning_system(self):
        """ì‹¤ì‹œê°„ í•™ìŠµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        test_name = "realtime_learning"
        logger.info(f"ğŸ§  {test_name} í…ŒìŠ¤íŠ¸ ì‹œì‘")

        try:
            start_time = time.time()

            # ì‹¤ì‹œê°„ í•™ìŠµ ì—”ì§„ ì¡°íšŒ
            engine = await get_realtime_engine()
            assert engine is not None, "ì‹¤ì‹œê°„ í•™ìŠµ ì—”ì§„ ì¡°íšŒ ì‹¤íŒ¨"

            # í…ŒìŠ¤íŠ¸ ìƒí˜¸ì‘ìš© ë°ì´í„°
            test_interactions = [
                {
                    "user_id": self.test_user_id,
                    "vehicle_id": self.test_vehicle_id,
                    "interaction_type": "view",
                    "context": {"duration_seconds": 30}
                },
                {
                    "user_id": self.test_user_id,
                    "vehicle_id": self.test_vehicle_id,
                    "interaction_type": "like",
                    "context": {"duration_seconds": 60}
                },
                {
                    "user_id": self.test_user_id,
                    "vehicle_id": self.test_vehicle_id,
                    "interaction_type": "inquiry",
                    "context": {"duration_seconds": 120}
                }
            ]

            # ìƒí˜¸ì‘ìš© ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
            processed_count = 0
            for interaction in test_interactions:
                success = await engine.process_user_interaction(interaction)
                if success:
                    processed_count += 1

            assert processed_count == len(test_interactions), f"ìƒí˜¸ì‘ìš© ì²˜ë¦¬ ì‹¤íŒ¨: {processed_count}/{len(test_interactions)}"

            # ì¶”ì²œ ìºì‹œ í…ŒìŠ¤íŠ¸
            cache = await get_recommendation_cache()
            assert cache is not None, "ì¶”ì²œ ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨"

            # ìºì‹œ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸
            test_recommendations = [
                {"vehicle_id": "test_001", "score": 0.9},
                {"vehicle_id": "test_002", "score": 0.8}
            ]

            await cache.update_user_recommendations(self.test_user_id, test_recommendations)

            # ìºì‹œ ì¡°íšŒ í…ŒìŠ¤íŠ¸
            cached_recommendations = await cache.get_user_recommendations(self.test_user_id)
            assert len(cached_recommendations) > 0, "ìºì‹œëœ ì¶”ì²œ ì¡°íšŒ ì‹¤íŒ¨"

            duration = time.time() - start_time
            self.test_results[test_name] = {
                "status": "passed",
                "duration_seconds": duration,
                "details": {
                    "interactions_processed": processed_count,
                    "cache_operations": True,
                    "engine_initialized": True
                }
            }

            logger.info(f"âœ… {test_name} í…ŒìŠ¤íŠ¸ í†µê³¼ ({duration:.2f}ì´ˆ)")

        except Exception as e:
            self.test_results[test_name] = {
                "status": "failed",
                "error": str(e),
                "duration_seconds": time.time() - start_time
            }
            logger.error(f"âŒ {test_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

    async def _test_gemini_multiagent(self):
        """Gemini ë©€í‹°ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸"""
        test_name = "gemini_multiagent"
        logger.info(f"ğŸ¤– {test_name} í…ŒìŠ¤íŠ¸ ì‹œì‘")

        try:
            start_time = time.time()

            # API í‚¤ í™•ì¸
            gemini_key = os.getenv('GEMINI_API_KEY')
            if not gemini_key:
                logger.warning("âš ï¸ GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ - ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ")
                self.test_results[test_name] = {
                    "status": "skipped",
                    "reason": "GEMINI_API_KEY not configured",
                    "duration_seconds": time.time() - start_time
                }
                return

            # ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì¡°íšŒ
            multi_agent = await get_gemini_multi_agent_system(gemini_key)
            assert multi_agent is not None, "ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì¡°íšŒ ì‹¤íŒ¨"

            # ì¶”ì²œ ìƒì„± í…ŒìŠ¤íŠ¸
            recommendations = await multi_agent.get_recommendations(
                user_id=self.test_user_id,
                context_data={"test_mode": True}
            )

            assert recommendations is not None, "ì¶”ì²œ ìƒì„± ì‹¤íŒ¨"
            assert "recommendations" in recommendations, "ì¶”ì²œ ê²°ê³¼ í˜•ì‹ ì˜¤ë¥˜"

            # ì—ì´ì „íŠ¸ ê¸°ì—¬ë„ í™•ì¸
            if "agent_contributions" in recommendations:
                contributions = recommendations["agent_contributions"]
                assert len(contributions) > 0, "ì—ì´ì „íŠ¸ ê¸°ì—¬ë„ ì •ë³´ ì—†ìŒ"

            duration = time.time() - start_time
            self.test_results[test_name] = {
                "status": "passed",
                "duration_seconds": duration,
                "details": {
                    "recommendations_generated": len(recommendations.get("recommendations", [])),
                    "agent_contributions": len(recommendations.get("agent_contributions", {})),
                    "confidence_score": recommendations.get("confidence_score", 0)
                }
            }

            logger.info(f"âœ… {test_name} í…ŒìŠ¤íŠ¸ í†µê³¼ ({duration:.2f}ì´ˆ)")

        except Exception as e:
            self.test_results[test_name] = {
                "status": "failed",
                "error": str(e),
                "duration_seconds": time.time() - start_time
            }
            logger.error(f"âŒ {test_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

    async def _test_mcp_server(self):
        """MCP ì„œë²„ í…ŒìŠ¤íŠ¸"""
        test_name = "mcp_server"
        logger.info(f"ğŸ”§ {test_name} í…ŒìŠ¤íŠ¸ ì‹œì‘")

        try:
            start_time = time.time()

            # MCP ì„œë²„ ì¡°íšŒ
            mcp_server = await get_carfinance_mcp_server()
            assert mcp_server is not None, "MCP ì„œë²„ ì¡°íšŒ ì‹¤íŒ¨"

            # ì„œë²„ ì •ë³´ í…ŒìŠ¤íŠ¸
            server_info = await mcp_server.get_server_info()
            assert "name" in server_info, "ì„œë²„ ì •ë³´ ì˜¤ë¥˜"
            assert "tools" in server_info, "ë„êµ¬ ëª©ë¡ ì •ë³´ ëˆ„ë½"

            # ë„êµ¬ ëª©ë¡ í…ŒìŠ¤íŠ¸
            tools = await mcp_server.list_tools()
            assert len(tools) > 0, "MCP ë„êµ¬ ì—†ìŒ"

            tool_names = [tool["name"] for tool in tools]
            expected_tools = [
                "carfinance__get_recommendations",
                "carfinance__analyze_behavior",
                "carfinance__track_interaction",
                "carfinance__compare_vehicles"
            ]

            for expected_tool in expected_tools:
                assert expected_tool in tool_names, f"í•„ìˆ˜ ë„êµ¬ ëˆ„ë½: {expected_tool}"

            # í—¬ìŠ¤ ì²´í¬ í…ŒìŠ¤íŠ¸
            health = await mcp_server.health_check()
            assert health["status"] in ["healthy", "unhealthy"], "í—¬ìŠ¤ ì²´í¬ ê²°ê³¼ í˜•ì‹ ì˜¤ë¥˜"

            # ë„êµ¬ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ (ìƒí˜¸ì‘ìš© ì¶”ì )
            track_result = await mcp_server.execute_tool(
                "carfinance__track_interaction",
                user_id=self.test_user_id,
                vehicle_id=self.test_vehicle_id,
                interaction_type="view"
            )

            assert track_result.success, f"ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {track_result.error}"

            duration = time.time() - start_time
            self.test_results[test_name] = {
                "status": "passed",
                "duration_seconds": duration,
                "details": {
                    "tools_available": len(tools),
                    "health_status": health["status"],
                    "tool_execution": track_result.success
                }
            }

            logger.info(f"âœ… {test_name} í…ŒìŠ¤íŠ¸ í†µê³¼ ({duration:.2f}ì´ˆ)")

        except Exception as e:
            self.test_results[test_name] = {
                "status": "failed",
                "error": str(e),
                "duration_seconds": time.time() - start_time
            }
            logger.error(f"âŒ {test_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

    async def _test_behavior_collection_strategy(self):
        """ì‚¬ìš©ì í–‰ë™ ìˆ˜ì§‘ ì „ëµ í…ŒìŠ¤íŠ¸"""
        test_name = "behavior_collection"
        logger.info(f"ğŸ“Š {test_name} í…ŒìŠ¤íŠ¸ ì‹œì‘")

        try:
            start_time = time.time()

            # ìˆ˜ì§‘ ì „ëµ ì¡°íšŒ
            strategy = await get_user_behavior_collection_strategy()
            assert strategy is not None, "í–‰ë™ ìˆ˜ì§‘ ì „ëµ ì¡°íšŒ ì‹¤íŒ¨"

            # í…ŒìŠ¤íŠ¸ ì´ë²¤íŠ¸ ë°ì´í„°
            test_events = [
                {
                    "user_id": self.test_user_id,
                    "event_type": "view",
                    "vehicle_id": self.test_vehicle_id,
                    "session_id": "test_session_001",
                    "duration_seconds": 30
                },
                {
                    "user_id": self.test_user_id,
                    "event_type": "inquiry",
                    "vehicle_id": self.test_vehicle_id,
                    "session_id": "test_session_001",
                    "duration_seconds": 120
                }
            ]

            # í–‰ë™ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
            collected_count = 0
            for event in test_events:
                success = await strategy.collect_behavior(event)
                if success:
                    collected_count += 1

            assert collected_count == len(test_events), f"í–‰ë™ ìˆ˜ì§‘ ì‹¤íŒ¨: {collected_count}/{len(test_events)}"

            # ë©”íŠ¸ë¦­ ì¡°íšŒ í…ŒìŠ¤íŠ¸
            metrics = await strategy.get_collection_metrics()
            assert "total_events_processed" in metrics, "ë©”íŠ¸ë¦­ ì •ë³´ ëˆ„ë½"
            assert metrics["total_events_processed"] >= collected_count, "ì²˜ë¦¬ëœ ì´ë²¤íŠ¸ ìˆ˜ ë¶ˆì¼ì¹˜"

            duration = time.time() - start_time
            self.test_results[test_name] = {
                "status": "passed",
                "duration_seconds": duration,
                "details": {
                    "events_collected": collected_count,
                    "total_processed": metrics["total_events_processed"],
                    "queue_sizes": metrics.get("queue_sizes", {})
                }
            }

            logger.info(f"âœ… {test_name} í…ŒìŠ¤íŠ¸ í†µê³¼ ({duration:.2f}ì´ˆ)")

        except Exception as e:
            self.test_results[test_name] = {
                "status": "failed",
                "error": str(e),
                "duration_seconds": time.time() - start_time
            }
            logger.error(f"âŒ {test_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

    async def _test_full_system_integration(self):
        """ì „ì²´ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸"""
        test_name = "full_integration"
        logger.info(f"ğŸ”„ {test_name} í…ŒìŠ¤íŠ¸ ì‹œì‘")

        try:
            start_time = time.time()

            # ì „ì²´ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸: ì‚¬ìš©ì í–‰ë™ â†’ ì‹¤ì‹œê°„ í•™ìŠµ â†’ ì¶”ì²œ ìƒì„± â†’ MCP ì„œë¹™

            # 1. ì‚¬ìš©ì í–‰ë™ ìˆ˜ì§‘
            strategy = await get_user_behavior_collection_strategy()

            behavior_event = {
                "user_id": self.test_user_id,
                "event_type": "inquiry",
                "vehicle_id": self.test_vehicle_id,
                "session_id": "integration_test_session",
                "duration_seconds": 180,
                "context": {"integration_test": True}
            }

            collection_success = await strategy.collect_behavior(behavior_event)
            assert collection_success, "í–‰ë™ ìˆ˜ì§‘ ì‹¤íŒ¨"

            # 2. ì‹¤ì‹œê°„ í•™ìŠµ ì²˜ë¦¬
            engine = await get_realtime_engine()
            learning_success = await engine.process_user_interaction({
                "user_id": self.test_user_id,
                "vehicle_id": self.test_vehicle_id,
                "interaction_type": "inquiry",
                "context": {"integration_test": True}
            })
            assert learning_success, "ì‹¤ì‹œê°„ í•™ìŠµ ì²˜ë¦¬ ì‹¤íŒ¨"

            # 3. MCPë¥¼ í†µí•œ ì¶”ì²œ ì¡°íšŒ
            mcp_server = await get_carfinance_mcp_server()
            recommendation_result = await mcp_server.execute_tool(
                "carfinance__get_recommendations",
                user_id=self.test_user_id,
                recommendation_count=3,
                use_sequential_thinking=False  # í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œëŠ” ë¹„í™œì„±í™”
            )

            # Gemini API í‚¤ê°€ ì—†ìœ¼ë©´ fallback ì¶”ì²œì´ ë°˜í™˜ë¨
            if recommendation_result.success:
                recommendations = recommendation_result.content
                assert "recommendations" in recommendations, "ì¶”ì²œ ê²°ê³¼ í˜•ì‹ ì˜¤ë¥˜"
            else:
                logger.warning("âš ï¸ ì¶”ì²œ ìƒì„± ì‹¤íŒ¨ (Gemini API í‚¤ ë¬¸ì œ ê°€ëŠ¥ì„±)")

            # 4. í–‰ë™ ë¶„ì„ í…ŒìŠ¤íŠ¸
            analysis_result = await mcp_server.execute_tool(
                "carfinance__analyze_behavior",
                user_id=self.test_user_id,
                analysis_period_days=1,
                include_predictions=False
            )

            assert analysis_result.success, f"í–‰ë™ ë¶„ì„ ì‹¤íŒ¨: {analysis_result.error}"

            # 5. ì°¨ëŸ‰ ë¹„êµ í…ŒìŠ¤íŠ¸
            compare_result = await mcp_server.execute_tool(
                "carfinance__compare_vehicles",
                vehicle_ids=[self.test_vehicle_id, "test_vehicle_002"],
                comparison_criteria=["price", "safety"],
                user_id=self.test_user_id
            )

            assert compare_result.success, f"ì°¨ëŸ‰ ë¹„êµ ì‹¤íŒ¨: {compare_result.error}"

            duration = time.time() - start_time
            self.test_results[test_name] = {
                "status": "passed",
                "duration_seconds": duration,
                "details": {
                    "behavior_collection": collection_success,
                    "realtime_learning": learning_success,
                    "recommendation_generation": recommendation_result.success,
                    "behavior_analysis": analysis_result.success,
                    "vehicle_comparison": compare_result.success
                }
            }

            logger.info(f"âœ… {test_name} í…ŒìŠ¤íŠ¸ í†µê³¼ ({duration:.2f}ì´ˆ)")

        except Exception as e:
            self.test_results[test_name] = {
                "status": "failed",
                "error": str(e),
                "duration_seconds": time.time() - start_time
            }
            logger.error(f"âŒ {test_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

    async def _test_system_performance(self):
        """ì‹œìŠ¤í…œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        test_name = "performance"
        logger.info(f"âš¡ {test_name} í…ŒìŠ¤íŠ¸ ì‹œì‘")

        try:
            start_time = time.time()

            # ë¶€í•˜ í…ŒìŠ¤íŠ¸: 100ê°œ ë™ì‹œ ì‚¬ìš©ì í–‰ë™ ì²˜ë¦¬
            strategy = await get_user_behavior_collection_strategy()

            # í…ŒìŠ¤íŠ¸ ì´ë²¤íŠ¸ ìƒì„±
            test_events = []
            for i in range(100):
                event = {
                    "user_id": f"perf_test_user_{i:03d}",
                    "event_type": "view",
                    "vehicle_id": f"perf_test_vehicle_{i % 10:03d}",
                    "session_id": f"perf_session_{i:03d}",
                    "duration_seconds": 30 + (i % 60)
                }
                test_events.append(event)

            # ë™ì‹œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
            performance_start = time.time()

            tasks = [strategy.collect_behavior(event) for event in test_events]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            performance_duration = time.time() - performance_start

            # ì„±ê³µë¥  ê³„ì‚°
            successful_count = sum(1 for result in results if result is True)
            success_rate = successful_count / len(test_events)

            # ì²˜ë¦¬ëŸ‰ ê³„ì‚° (events per second)
            throughput = len(test_events) / performance_duration

            assert success_rate >= 0.9, f"ì„±ê³µë¥  ë‚®ìŒ: {success_rate:.2%}"
            assert throughput >= 50, f"ì²˜ë¦¬ëŸ‰ ë‚®ìŒ: {throughput:.2f} events/sec"

            duration = time.time() - start_time
            self.test_results[test_name] = {
                "status": "passed",
                "duration_seconds": duration,
                "details": {
                    "events_tested": len(test_events),
                    "success_rate": success_rate,
                    "throughput_events_per_second": throughput,
                    "processing_duration": performance_duration
                }
            }

            logger.info(f"âœ… {test_name} í…ŒìŠ¤íŠ¸ í†µê³¼ ({duration:.2f}ì´ˆ)")
            logger.info(f"ğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­: {successful_count}/{len(test_events)} ì„±ê³µ, {throughput:.2f} events/sec")

        except Exception as e:
            self.test_results[test_name] = {
                "status": "failed",
                "error": str(e),
                "duration_seconds": time.time() - start_time
            }
            logger.error(f"âŒ {test_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

    async def _cleanup_test_data(self):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬"""
        try:
            if self.db_manager:
                async with self.db_manager.get_connection() as conn:
                    # í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ì‚­ì œ
                    await conn.execute(
                        "DELETE FROM user_interactions WHERE user_id LIKE 'test_%' OR user_id LIKE 'perf_test_%'"
                    )

                    logger.info("ğŸ§¹ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def generate_test_report(self) -> str:
        """í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        report = []
        report.append("=" * 60)
        report.append("CarFinanceAI í†µí•© í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ")
        report.append("=" * 60)
        report.append(f"ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")

        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        total_tests = len([k for k in self.test_results.keys() if k != "overall"])
        passed_tests = len([v for v in self.test_results.values() if v.get("status") == "passed"])
        failed_tests = len([v for v in self.test_results.values() if v.get("status") == "failed"])
        skipped_tests = len([v for v in self.test_results.values() if v.get("status") == "skipped"])

        report.append("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        report.append(f"  ì´ í…ŒìŠ¤íŠ¸: {total_tests}")
        report.append(f"  í†µê³¼: {passed_tests} âœ…")
        report.append(f"  ì‹¤íŒ¨: {failed_tests} âŒ")
        report.append(f"  ê±´ë„ˆëœ€: {skipped_tests} â­ï¸")
        report.append("")

        # ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼
        report.append("ğŸ“‹ ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        for test_name, result in self.test_results.items():
            if test_name == "overall":
                continue

            status_icon = {
                "passed": "âœ…",
                "failed": "âŒ",
                "skipped": "â­ï¸"
            }.get(result["status"], "â“")

            duration = result.get("duration_seconds", 0)
            report.append(f"  {status_icon} {test_name}: {result['status']} ({duration:.2f}ì´ˆ)")

            if result["status"] == "failed":
                report.append(f"    ì˜¤ë¥˜: {result.get('error', 'Unknown error')}")

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        if "performance" in self.test_results and self.test_results["performance"]["status"] == "passed":
            perf = self.test_results["performance"]["details"]
            report.append("")
            report.append("âš¡ ì„±ëŠ¥ ë©”íŠ¸ë¦­")
            report.append(f"  ì²˜ë¦¬ëŸ‰: {perf.get('throughput_events_per_second', 0):.2f} events/sec")
            report.append(f"  ì„±ê³µë¥ : {perf.get('success_rate', 0):.1%}")

        report.append("")
        report.append("=" * 60)

        return "\n".join(report)

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜
async def run_integration_tests():
    """í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    test_suite = IntegrationTestSuite()
    results = await test_suite.run_all_tests()

    # í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ì¶œë ¥
    report = test_suite.generate_test_report()
    print(report)

    # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"claudedocs/integration_test_report_{timestamp}.md"

    os.makedirs("claudedocs", exist_ok=True)
    with open(report_file, "w", encoding="utf-8") as f:
        f.write("# CarFinanceAI í†µí•© í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ\n\n")
        f.write(f"ì‹¤í–‰ ì‹œê°„: {datetime.now().isoformat()}\n\n")
        f.write("```\n")
        f.write(report)
        f.write("\n```\n\n")
        f.write("## ìƒì„¸ ê²°ê³¼\n\n")
        f.write("```json\n")
        f.write(json.dumps(results, ensure_ascii=False, indent=2))
        f.write("\n```\n")

    logger.info(f"ğŸ“„ í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ì €ì¥: {report_file}")

    return results

if __name__ == "__main__":
    # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(run_integration_tests())