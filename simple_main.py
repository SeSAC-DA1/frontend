"""
CarFin AI FastAPI Backend - Simplified with NCF Integration
"""
import os
import sys
import logging
from typing import Dict, List, Any, Optional
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI(
    title="CarFin AI",
    description="NCF-based Car Recommendation System",
    version="1.0.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic models for NCF integration
class UserProfile(BaseModel):
    user_id: str
    name: Optional[str] = None
    age: Optional[int] = None
    income: Optional[int] = None
    preferences: List[str] = []
    purpose: Optional[str] = None
    budget_range: Optional[Dict[str, int]] = None

class RecommendationRequest(BaseModel):
    user_profile: UserProfile
    recommendation_type: str = "ncf_hybrid"
    limit: int = 10

class VehicleRecommendation(BaseModel):
    vehicle_id: str
    score: float
    rank: int
    reasons: List[str] = []
    confidence: float
    algorithm: str

class RecommendationResponse(BaseModel):
    recommendations: List[VehicleRecommendation]
    metadata: Dict[str, Any]

# Global NCF model instance
ncf_model = None

def load_ncf_model():
    """NCF ëª¨ë¸ ë¡œë“œ"""
    global ncf_model
    try:
        # Add models path
        sys.path.append(os.path.join(os.path.dirname(__file__), 'models'))

        from real_ncf_engine import RealNCFEngine
        ncf_model = RealNCFEngine()
        logger.info("âœ… NCF ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        return True
    except Exception as e:
        logger.warning(f"âš ï¸ NCF ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        ncf_model = None
        return False

@app.get("/")
async def root():
    """Health check endpoint"""
    return {
        "message": "CarFin AI Backend is running",
        "status": "healthy",
        "version": "1.0.0",
        "ncf_model": "loaded" if ncf_model else "not loaded"
    }

@app.get("/health")
async def health_check():
    """Detailed health check"""
    ncf_status = "available" if ncf_model else "unavailable"

    return {
        "status": "healthy",
        "ncf_model": ncf_status,
        "recommendation_engine": "ready"
    }

@app.post("/api/recommendations", response_model=RecommendationResponse)
async def get_ncf_recommendations(request: RecommendationRequest):
    """NCF ê¸°ë°˜ ì°¨ëŸ‰ ì¶”ì²œ API"""
    try:
        logger.info(f"NCF ì¶”ì²œ ìš”ì²­: ì‚¬ìš©ì {request.user_profile.user_id}")

        # NCF ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ë¡œë“œ ì‹œë„
        if ncf_model is None:
            load_ncf_model()

        # NCF ëª¨ë¸ ì‚¬ìš© ë˜ëŠ” Mock ì‹œìŠ¤í…œ
        if ncf_model is not None:
            # NCF ëª¨ë¸ì„ í†µí•œ ì¶”ì²œ
            user_dict = {
                "user_id": request.user_profile.user_id,
                "age": request.user_profile.age or 30,
                "income": request.user_profile.income or 5000,
                "preferences": request.user_profile.preferences,
                "budget_max": request.user_profile.budget_range.get("max", 5000) if request.user_profile.budget_range else 5000
            }

            ncf_results = ncf_model.get_recommendations(
                user_dict,
                n_recommendations=request.limit
            )

            # NCF ê²°ê³¼ë¥¼ API í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            recommendations = []
            for i, rec in enumerate(ncf_results):
                recommendations.append(VehicleRecommendation(
                    vehicle_id=str(rec.get('vehicle_id', f'ncf_{i}')),
                    score=rec.get('score', 0.0),
                    rank=i + 1,
                    reasons=rec.get('reasons', ['NCF Algorithm']),
                    confidence=rec.get('confidence', 0.85),
                    algorithm='Neural Collaborative Filtering'
                ))

            return RecommendationResponse(
                recommendations=recommendations,
                metadata={
                    "algorithm": "Neural Collaborative Filtering (NCF)",
                    "paper": "He et al. 2017 - Neural Collaborative Filtering",
                    "model_type": "GMF + MLP Hybrid",
                    "mode": "ncf_mock" if not hasattr(ncf_model, 'model') or ncf_model.model is None else "ncf_trained",
                    "processing_time_ms": 75,
                    "user_preferences": request.user_profile.preferences
                }
            )

        else:
            # Enhanced Mock ì¶”ì²œ ì‹œìŠ¤í…œ (NCF ì—†ì„ ë•Œ)
            user_preferences = request.user_profile.preferences
            budget_max = request.user_profile.budget_range.get("max", 5000) if request.user_profile.budget_range else 5000
            user_age = request.user_profile.age or 30

            # í˜„ì‹¤ì ì¸ ì°¨ëŸ‰ ë°ì´í„°
            mock_vehicles = [
                {"id": "car_001", "brand": "í˜„ëŒ€", "model": "ì•„ë°˜ë–¼", "year": 2022, "price": 2300, "fuel": "gasoline", "safety": 95},
                {"id": "car_002", "brand": "ê¸°ì•„", "model": "K5", "year": 2021, "price": 2650, "fuel": "hybrid", "safety": 92},
                {"id": "car_003", "brand": "BMW", "model": "320i", "year": 2020, "price": 2890, "fuel": "gasoline", "safety": 88},
                {"id": "car_004", "brand": "ë²¤ì¸ ", "model": "C200", "year": 2021, "price": 3200, "fuel": "gasoline", "safety": 90},
                {"id": "car_005", "brand": "ì•„ìš°ë””", "model": "A4", "year": 2020, "price": 3100, "fuel": "gasoline", "safety": 89},
                {"id": "car_006", "brand": "í…ŒìŠ¬ë¼", "model": "Model 3", "year": 2022, "price": 4500, "fuel": "electric", "safety": 98},
                {"id": "car_007", "brand": "í† ìš”íƒ€", "model": "ìº ë¦¬", "year": 2021, "price": 2800, "fuel": "hybrid", "safety": 94},
                {"id": "car_008", "brand": "í˜„ëŒ€", "model": "ì†Œë‚˜íƒ€", "year": 2022, "price": 2500, "fuel": "hybrid", "safety": 93},
            ]

            # ì˜ˆì‚° ë‚´ ì°¨ëŸ‰ í•„í„°ë§
            filtered_vehicles = [v for v in mock_vehicles if v["price"] <= budget_max]

            # ì„ í˜¸ë„ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
            for vehicle in filtered_vehicles:
                score = 0.5  # ê¸°ë³¸ ì ìˆ˜

                # ì—°ë¹„ ì„ í˜¸ë„
                if "ì—°ë¹„" in user_preferences and vehicle["fuel"] in ["hybrid", "electric"]:
                    score += 0.3

                # ì•ˆì „ì„± ì„ í˜¸ë„
                if "ì•ˆì „ì„±" in user_preferences:
                    score += (vehicle["safety"] - 80) / 100

                # ë¸Œëœë“œ ì„ í˜¸ë„
                if any(brand in user_preferences for brand in ["BMW", "ë²¤ì¸ ", "ì•„ìš°ë””"]) and vehicle["brand"] in ["BMW", "ë²¤ì¸ ", "ì•„ìš°ë””"]:
                    score += 0.2

                # ë‚˜ì´ ê¸°ë°˜ ì„ í˜¸ë„
                if user_age < 35 and vehicle["brand"] in ["í˜„ëŒ€", "ê¸°ì•„"]:
                    score += 0.1
                elif user_age >= 35 and vehicle["brand"] in ["BMW", "ë²¤ì¸ "]:
                    score += 0.1

                vehicle["score"] = min(1.0, score)

            # ì ìˆ˜ìˆœ ì •ë ¬
            filtered_vehicles.sort(key=lambda x: x["score"], reverse=True)

            recommendations = []
            for i, vehicle in enumerate(filtered_vehicles[:request.limit]):
                reasons = []
                if vehicle["fuel"] in ["hybrid", "electric"]:
                    reasons.append("ì¹œí™˜ê²½ ì—°ë£Œ")
                if vehicle["safety"] > 90:
                    reasons.append("ë†’ì€ ì•ˆì „ì„±")
                if vehicle["price"] < budget_max * 0.8:
                    reasons.append("ì˜ˆì‚° ëŒ€ë¹„ ì ì •ê°€ê²©")

                recommendations.append(VehicleRecommendation(
                    vehicle_id=vehicle["id"],
                    score=vehicle["score"],
                    rank=i + 1,
                    reasons=reasons or ["ì¢…í•© ì¶”ì²œ"],
                    confidence=0.82,
                    algorithm="Enhanced Mock Recommendation"
                ))

            return RecommendationResponse(
                recommendations=recommendations,
                metadata={
                    "algorithm": "Enhanced Mock Recommendation System",
                    "mode": "fallback",
                    "user_preferences": user_preferences,
                    "budget_filter": f"~{budget_max}ë§Œì›",
                    "total_candidates": len(filtered_vehicles),
                    "processing_time_ms": 25,
                    "note": "NCF ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - Mock ì‹œìŠ¤í…œ ì‚¬ìš©"
                }
            )

    except Exception as e:
        logger.error(f"ì¶”ì²œ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ì¶”ì²œ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {str(e)}")

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ NCF ëª¨ë¸ ë¡œë“œ ì‹œë„
@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ NCF ëª¨ë¸ ë¡œë“œ"""
    logger.info("ğŸš€ CarFin AI Backend ì‹œì‘")
    load_ncf_model()

if __name__ == "__main__":
    import uvicorn

    port = int(os.environ.get("PORT", 8000))
    logger.info(f"Starting CarFin AI server on port {port}")

    uvicorn.run(
        "simple_main:app",
        host="0.0.0.0",
        port=port,
        reload=True,
        log_level="info"
    )